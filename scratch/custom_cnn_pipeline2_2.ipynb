{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Triton\n",
        "!pip install --upgrade triton\n",
        "\n",
        "# Import with updated autocast and GradScaler usage\n",
        "from torch.amp import autocast\n",
        "from torch.amp import GradScaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf0NDoa_hyFX",
        "outputId": "9b3ed12e-0661-439c-eb0d-50d37471ef95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torchvision.models.resnet import BasicBlock\n",
        "\n",
        "# Optimization 1: Use torch.compile for performance boost on PyTorch 2.0+\n",
        "try:\n",
        "    TORCH_COMPILE_AVAILABLE = hasattr(torch, 'compile')\n",
        "except:\n",
        "    TORCH_COMPILE_AVAILABLE = False\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "cHZ51ZzGcvmv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedCustomCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    An improved CNN architecture for action recognition with enhanced feature extraction,\n",
        "    better regularization, and modern architectural choices for improved generalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=40):\n",
        "        super(OptimizedCustomCNN, self).__init__()\n",
        "\n",
        "        # Enhanced feature extraction layers with regularization\n",
        "        self.features = nn.Sequential(\n",
        "            # Initial block: larger kernel for better spatial understanding\n",
        "            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.2),  # Spatial dropout to prevent feature co-adaptation\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            # First feature block with residual-style double convolution\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Additional conv for better feature learning\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Second block with increased channel capacity\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.3),  # Increased dropout as we go deeper\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),  # Double conv for richer features\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Third block for high-level feature extraction\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.4),  # Highest dropout in deepest conv layers\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Global average pooling for better generalization\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Enhanced classifier with multiple layers and strong regularization\n",
        "        self.classifier = nn.Sequential(\n",
        "            # First classifier block\n",
        "            nn.Linear(256, 512),  # Expand features first\n",
        "            nn.BatchNorm1d(512),  # Normalize activations\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),  # Heavy dropout for regularization\n",
        "\n",
        "            # Second classifier block for better feature abstraction\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            # Final classification layer\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights using modern techniques\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize network weights using modern techniques for better training dynamics:\n",
        "        - Kaiming initialization for conv layers (considering ReLU)\n",
        "        - Constant initialization for batch norm\n",
        "        - Normal initialization for linear layers\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract hierarchical features\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = self.global_pool(x)\n",
        "\n",
        "        # Flatten and classify\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vqZXfjTwIFdd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedTrainer:\n",
        "    def __init__(self, train_loader, val_loader, device, num_epochs=20):\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        self.model = OptimizedCustomCNN(num_classes=40).to(device)\n",
        "\n",
        "        # Warmup compilation\n",
        "        if TORCH_COMPILE_AVAILABLE:\n",
        "          self.model = torch.compile(self.model, mode='reduce-overhead')\n",
        "          print(\"Model compiled with torch.compile()\")\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=2e-3,\n",
        "            weight_decay=0.1\n",
        "        )\n",
        "\n",
        "        self.scheduler = OneCycleLR(\n",
        "            self.optimizer,\n",
        "            max_lr=2e-3,\n",
        "            epochs=num_epochs,\n",
        "            steps_per_epoch=len(train_loader),\n",
        "            pct_start=0.2,\n",
        "            div_factor=25,\n",
        "            final_div_factor=1000\n",
        "        )\n",
        "\n",
        "        # Add scaler initialization\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "\n",
        "        self.history = {\n",
        "            'train_loss': [], 'train_acc': [],\n",
        "            'val_loss': [], 'val_acc': [],\n",
        "            'learning_rates': []\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        for batch_idx, (inputs, labels, _) in enumerate(self.train_loader):\n",
        "            inputs = inputs.to(self.device, non_blocking=True)\n",
        "            labels = labels.to(self.device, non_blocking=True)\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            self.scaler.unscale_(self.optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.scaler.step(self.optimizer)\n",
        "\n",
        "            self.scaler.unscale_(self.optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                batch_time = time.time() - batch_start_time\n",
        "                current_loss = running_loss / (batch_idx + 1)\n",
        "                current_acc = 100. * correct / total\n",
        "                gpu_mem = torch.cuda.memory_allocated() / 1024**2\n",
        "\n",
        "                print(f'Batch {batch_idx}/{len(self.train_loader)} | '\n",
        "                      f'Time: {batch_time:.2f}s | '\n",
        "                      f'Loss: {current_loss:.4f} | '\n",
        "                      f'Acc: {current_acc:.2f}% | '\n",
        "                      f'GPU Mem: {gpu_mem:.0f}MB')\n",
        "\n",
        "                batch_start_time = time.time()\n",
        "\n",
        "        epoch_loss = running_loss / len(self.train_loader)\n",
        "        epoch_acc = 100. * correct / total\n",
        "\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels, _ in tqdm(self.val_loader, desc=\"Validating\"):\n",
        "            inputs = inputs.to(self.device, non_blocking=True)\n",
        "            labels = labels.to(self.device, non_blocking=True)\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss = running_loss / len(self.val_loader)\n",
        "        val_acc = 100. * correct / total\n",
        "\n",
        "        return val_loss, val_acc\n",
        "\n",
        "    def train(self, save_dir='model_checkpoints', validate_every=15):\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        best_val_acc = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\nStarting training...\")\n",
        "        print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "            # Training phase\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "\n",
        "            # Update training history\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['learning_rates'].append(\n",
        "                self.optimizer.param_groups[0]['lr']\n",
        "            )\n",
        "\n",
        "            # Validation phase\n",
        "            if (epoch + 1) % validate_every == 0 or (epoch + 1) == self.num_epochs:\n",
        "                val_loss, val_acc = self.validate()\n",
        "                self.history['val_loss'].append(val_loss)\n",
        "                self.history['val_acc'].append(val_acc)\n",
        "\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': self.model.state_dict(),\n",
        "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                        'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "                        'val_acc': val_acc,\n",
        "                        'history': self.history\n",
        "                    }, os.path.join(save_dir, 'best_model.pth'))\n",
        "\n",
        "                print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_acc:.2f}%\")\n",
        "            else:\n",
        "                self.history['val_loss'].append(None)\n",
        "                self.history['val_acc'].append(None)\n",
        "\n",
        "            # Print epoch summary\n",
        "            total_time = time.time() - start_time\n",
        "            hours = int(total_time // 3600)\n",
        "            minutes = int((total_time % 3600) // 60)\n",
        "            seconds = int(total_time % 60)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "            print(f\"Training Loss: {train_loss:.4f}\")\n",
        "            print(f\"Training Accuracy: {train_acc:.2f}%\")\n",
        "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
        "            print(f\"Total Training Time: {hours}h {minutes}m {seconds}s\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "qA7HmBNllo--"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stanford40Dataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the Stanford40 dataset.\n",
        "\n",
        "        Args:\n",
        "            root_dir: The root directory containing the dataset\n",
        "            split: Either 'train' or 'test'\n",
        "            transform: Optional transformations to apply to the images\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "\n",
        "        # Load and process action classes\n",
        "        actions_file = os.path.join(root_dir, 'ImageSplits/actions.txt')\n",
        "        if not os.path.exists(actions_file):\n",
        "            raise ValueError(f\"Actions file not found at: {actions_file}\")\n",
        "\n",
        "        with open(actions_file, 'r') as f:\n",
        "            # Skip the first line (count) and clean up action names\n",
        "            lines = f.readlines()[1:]\n",
        "            self.classes = [line.split()[0].strip() for line in lines]\n",
        "\n",
        "        print(f\"Found {len(self.classes)} action classes\")\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        # Build list of images\n",
        "        self.images = []\n",
        "        for action in self.classes:\n",
        "            split_file = os.path.join(root_dir, f'ImageSplits/{action}_{split}.txt')\n",
        "\n",
        "            if not os.path.exists(split_file):\n",
        "                print(f\"Warning: Split file not found: {split_file}\")\n",
        "                continue\n",
        "\n",
        "            with open(split_file, 'r') as f:\n",
        "                # Important: Handle the image IDs properly\n",
        "                # Remove any file extension if present in the text file\n",
        "                image_ids = [os.path.splitext(line.strip())[0] for line in f.readlines()]\n",
        "                self.images.extend([(action, img_id) for img_id in image_ids])\n",
        "\n",
        "        print(f\"Loaded {len(self.images)} images for {split} split\")\n",
        "\n",
        "        # Verify first few images exist\n",
        "        print(\"\\nVerifying first few images...\")\n",
        "        for i, (action, img_id) in enumerate(self.images[:5]):\n",
        "            img_path = os.path.join(self.root_dir, 'JPEGImages', f'{img_id}.jpg')\n",
        "            exists = os.path.exists(img_path)\n",
        "            print(f\"Image {i+1}: {img_path} - {'Found' if exists else 'Not found'}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the item to retrieve\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, label, dummy_bbox)\n",
        "        \"\"\"\n",
        "        action, img_id = self.images[idx]\n",
        "\n",
        "        # Construct image path - ensure we add only one .jpg extension\n",
        "        img_path = os.path.join(self.root_dir, 'JPEGImages', f'{img_id}.jpg')\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            # Provide detailed error message\n",
        "            raise FileNotFoundError(\n",
        "                f\"Image not found: {img_path}\\n\"\n",
        "                f\"Action: {action}\\n\"\n",
        "                f\"Image ID: {img_id}\"\n",
        "            )\n",
        "\n",
        "        # Load and process image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, self.class_to_idx[action], torch.tensor([0, 0, 0, 0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "metadata": {
        "id": "zHgI4CjBeDu4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # First, spatial transforms that work with PIL Images\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.2,\n",
        "        contrast=0.2,\n",
        "        saturation=0.2,\n",
        "        hue=0.1\n",
        "    ),\n",
        "    # Convert to tensor (must come before tensor-based operations)\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Tensor-based operations come after ToTensor()\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.2)  # Works on tensors, so must come after ToTensor\n",
        "])\n",
        "\n",
        "# Simple validation transform\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Set up datasets with appropriate transforms\n",
        "print(\"Setting up datasets...\")\n",
        "dataset_path = '/content/drive/MyDrive/Stanford40'\n",
        "train_dataset = Stanford40Dataset(dataset_path, split='train', transform=train_transform)\n",
        "val_dataset = Stanford40Dataset(dataset_path, split='test', transform=val_transform)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=4,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        ")\n",
        "\n",
        "# Set up device and create directories\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results_dir = '/content/drive/MyDrive/training_results'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Print system information\n",
        "print(\"\\nSystem Information:\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**2:.1f}MB\")\n",
        "\n",
        "# Create model and trainer\n",
        "model = OptimizedCustomCNN().to(device)\n",
        "trainer = OptimizedTrainer(train_loader, val_loader, device)\n",
        "\n",
        "# Print model architecture summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Start training with periodic validation\n",
        "print(\"\\nStarting training process...\")\n",
        "start_time = time.time()\n",
        "history = trainer.train(\n",
        "    save_dir=results_dir,\n",
        "    validate_every=15  # Validate every 3 epochs\n",
        ")\n",
        "\n",
        "# Print final training summary\n",
        "total_time = time.time() - start_time\n",
        "hours = int(total_time // 3600)\n",
        "minutes = int((total_time % 3600) // 60)\n",
        "seconds = int(total_time % 60)\n",
        "\n",
        "print(\"\\nTraining Summary:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total training time: {hours}h {minutes}m {seconds}s\")\n",
        "print(f\"Best validation accuracy: {max(filter(None, history['val_acc'])):.2f}%\")\n",
        "print(f\"Final training accuracy: {history['train_acc'][-1]:.2f}%\")\n",
        "\n",
        "# Save training curves\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot training/validation loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot([i for i, v in enumerate(history['val_loss']) if v is not None],\n",
        "             [v for v in history['val_loss'] if v is not None],\n",
        "             'o-', label='Validation Loss')\n",
        "    plt.title('Loss Curves')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training/validation accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot([i for i, v in enumerate(history['val_acc']) if v is not None],\n",
        "             [v for v in history['val_acc'] if v is not None],\n",
        "             'o-', label='Validation Accuracy')\n",
        "    plt.title('Accuracy Curves')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'training_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\nTraining curves saved to {results_dir}/training_curves.png\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not save training curves: {str(e)}\")\n",
        "\n",
        "print(\"\\nTraining completed! All results saved to the 'training_results' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NQMe1ZGxdnPD",
        "outputId": "2cd0f7a6-0c62-4191-d24b-096f1cba3d27"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up datasets...\n",
            "Found 40 action classes\n",
            "Loaded 4000 images for train split\n",
            "\n",
            "Verifying first few images...\n",
            "Image 1: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_177.jpg - Found\n",
            "Image 2: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_119.jpg - Found\n",
            "Image 3: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_087.jpg - Found\n",
            "Image 4: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_228.jpg - Found\n",
            "Image 5: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_260.jpg - Found\n",
            "Found 40 action classes\n",
            "Loaded 5532 images for test split\n",
            "\n",
            "Verifying first few images...\n",
            "Image 1: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_254.jpg - Found\n",
            "Image 2: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_073.jpg - Found\n",
            "Image 3: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_123.jpg - Found\n",
            "Image 4: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_072.jpg - Found\n",
            "Image 5: /content/drive/MyDrive/Stanford40/JPEGImages/applauding_036.jpg - Found\n",
            "Training samples: 4000\n",
            "Validation samples: 5532\n",
            "\n",
            "System Information:\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Initial GPU Memory: 915.7MB\n",
            "Model compiled with torch.compile()\n",
            "\n",
            "Model Architecture:\n",
            "Total parameters: 1,443,432\n",
            "Trainable parameters: 1,443,432\n",
            "\n",
            "Starting training process...\n",
            "\n",
            "Starting training...\n",
            "Start time: 2024-12-30 15:58:10\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "unscale_() is being called after step().",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-cf5690d70e66>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training process...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m history = trainer.train(\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mvalidate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m  \u001b[0;31m# Validate every 3 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-cd45ca1f9b04>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, save_dir, validate_every)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# Update training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-cd45ca1f9b04>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    329\u001b[0m             )\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unscale_() is being called after step().\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: unscale_() is being called after step()."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN6LYiDuIyOc",
        "outputId": "8668a6c4-7c4a-405b-a00d-aa0be1111996"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}